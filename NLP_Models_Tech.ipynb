{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQPbxfb9bR900mgjeNVi13",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahzadahmad7/Natural-Language-Processing/blob/main/NLP_Models_Tech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words(BoW) model** is used to preprocess the text or documentations. It converts the documents\n",
        "into a bag of words, which keeps a count of the total occurrences of most frequently used words.\n",
        "Bag-of-Words is one of the most used methods to transform tokens into a set of features."
      ],
      "metadata": {
        "id": "piVBlLXe6MRA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvvxvaP14OXb",
        "outputId": "c0c5ac1f-403a-4e22-b226-00d6ae94008f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "    coding: 1\n",
            "    love: 1\n",
            "Document 2:\n",
            "    coding: 1\n",
            "    fun: 1\n",
            "    is: 1\n",
            "Document 3:\n",
            "    interesting: 1\n",
            "    is: 1\n",
            "    learning: 1\n",
            "    machine: 1\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"I love coding\",\n",
        "    \"Coding is fun\",\n",
        "    \"Machine learning is interesting\"\n",
        "]\n",
        "\n",
        "# Create the BoW model\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the BoW representation of each document\n",
        "for i in range(len(documents)):\n",
        "    print(f\"Document {i+1}:\")\n",
        "    for j in range(len(feature_names)):\n",
        "        word = feature_names[j]\n",
        "        count = X[i, j]\n",
        "        if count > 0:\n",
        "            print(f\"    {word}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term Frequency and Inverse Document Frequency is abbreviated as **TF-IDF**.\n",
        "1. Information retrieval (IR) or summarization scores are measured by this.  2. TF-IDF is additionally used to determine how pertinent a term is in a particular document.  3- Steps to multiplying two measures to determine the TF-IDF:   i. The frequency of a word in a document; ii. The word's inverse document frequency across a set of documents; 3.\n",
        "\n",
        "\n",
        "**Why do we require the TF-IDF?**\n",
        "1. TF-IDF aids in determining a word's significance within the context of the corpus of documents. TFIDF considers the frequency of the word in the document, offset by the number of documents included in the corpus.  TF is calculated by dividing a term's frequency by the total number of terms in the document.  3- The IDF is calculated by calculating the logarithm of the quotient produced by dividing the total number of documents by the number of documents containing the phrase.  4- The result of multiplying the two variables TF and IDF is then Tf.idf.\n"
      ],
      "metadata": {
        "id": "jHRX17GXFtsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"I love coding\",\n",
        "    \"Coding is fun\",\n",
        "    \"Machine learning is interesting\"\n",
        "]\n",
        "\n",
        "# Create the TF-IDF model\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the TF-IDF representation of each document\n",
        "for i in range(len(documents)):\n",
        "    print(f\"Document {i+1}:\")\n",
        "    for j in range(len(feature_names)):\n",
        "        word = feature_names[j]\n",
        "        tfidf_score = X[i, j]\n",
        "        if tfidf_score > 0:\n",
        "            print(f\"    {word}: {tfidf_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJoql1uq6pWN",
        "outputId": "6cc991a4-28da-48af-e733-5e2cfc32675c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "    coding: 0.6053485081062916\n",
            "    love: 0.7959605415681652\n",
            "Document 2:\n",
            "    coding: 0.5178561161676974\n",
            "    fun: 0.680918560398684\n",
            "    is: 0.5178561161676974\n",
            "Document 3:\n",
            "    interesting: 0.5286346066596935\n",
            "    is: 0.4020402441612698\n",
            "    learning: 0.5286346066596935\n",
            "    machine: 0.5286346066596935\n"
          ]
        }
      ]
    }
  ]
}